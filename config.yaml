# Configuration for ML Compiler Benchmark

benchmark:
  warmup_iterations: 10      # Iterations to skip (GPU warmup, JIT compilation)
  measured_iterations: 100   # Iterations to measure and report

models:
  - name: resnet50
    input_shape: [3, 224, 224]  # ImageNet standard (C, H, W) for vision models
    batch_sizes: [1, 32]        # Single image vs batched
    precision: fp32             # fp32, fp16, or int8
  - name: mobilenet_v3_large
    input_shape: [3, 224, 224]
    batch_sizes: [1, 32]
    precision: fp32
  - name: bert_base
    max_length: 128             # Sequence length for NLP models
    batch_sizes: [1, 8]         # Smaller batches for NLP models
    precision: fp32
  - name: gpt2
    max_length: 128             # Sequence length for NLP models
    batch_sizes: [1, 4]         # Even smaller batches for GPT-2
    precision: fp32

compilers:
  # PyTorch compilers
  - pytorch_eager      # Baseline (no compilation, reference implementation)
  - torchscript        # TorchScript JIT compiler (works on P100!)
  # - torch_inductor    # TorchInductor (doesn't work on P100, requires newer GPU)
  
  # Cross-platform compilers
  - onnx_runtime       # ONNX Runtime with GPU support (CUDAExecutionProvider)
  
  # TVM compilers (requires apache-tvm installation)
  # - tvm               # Apache TVM compiler (standard optimization)
  # - tvm_autotuned     # TVM with autotuning (slower compilation, better performance)
  
  # TensorRT compilers (requires NVIDIA TensorRT installation)
  # Note: TensorRT may have limitations on P100 (compute capability 6.0)
  # - tensorrt          # TensorRT FP32 (standard precision)
  # - tensorrt_fp16     # TensorRT FP16 (faster, may have accuracy loss)

ray:
  enabled: false              # Enable distributed execution with Ray
  num_gpus: null              # Auto-detect if null, otherwise specify number
  num_cpus: null              # Auto-detect if null
  head_address: null          # Connect to existing cluster if provided (for K8s)
  resources_per_task:
    num_gpus: 1               # GPUs per benchmark task
    num_cpus: 2               # CPUs per benchmark task

profiling:
  enabled: false              # Enable NVIDIA Nsight Systems profiling
  output_dir: results/profiles  # Directory for profiling reports
  profile_iterations: 10      # Number of iterations to profile (subset of measured iterations)

monitoring:
  prometheus:
    enabled: true             # Enable Prometheus metrics export
    port: 8000                # HTTP port for metrics endpoint

output:
  format: csv
  save_path: results/
